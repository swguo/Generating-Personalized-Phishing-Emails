{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swguo/Generating-Personalized-Phishing-Emails/blob/main/GPT2_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRf9FEQgMbuT",
        "outputId": "c5040814-eb91-411c-807d-1040d53712ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 68 ms, sys: 17.4 ms, total: 85.3 ms\n",
            "Wall time: 9.18 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install transformers==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z6a4fA_zMeX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c663f037-354e-4336-b536-aa52db2f01f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 16 07:02:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7BY1Wum1Mg78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2e958a-b65d-419a-8019-6e73914d9d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 1.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, \\\n",
        "                         Trainer, TrainingArguments\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "from IPython.display import clear_output\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxuGs9jPMrNv"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3u1DPWpiMqnd"
      },
      "outputs": [],
      "source": [
        "DEBUG           = False\n",
        "\n",
        "INPUT_DIR       = 'articles'\n",
        "\n",
        "USE_APEX        = True\n",
        "APEX_OPT_LEVEL  = 'O1'\n",
        "\n",
        "MODEL           = 'gpt2-medium' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",                    \n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\"}\n",
        "                    \n",
        "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
        "\n",
        "TRAIN_SIZE      = 0.8\n",
        "\n",
        "if USE_APEX:\n",
        "    TRAIN_BATCHSIZE = 4\n",
        "    BATCH_UPDATE    = 16\n",
        "else:\n",
        "    TRAIN_BATCHSIZE = 2\n",
        "    BATCH_UPDATE    = 32\n",
        "\n",
        "EPOCHS          = 4\n",
        "LR              = 5e-4\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_ARpqRsoMvaW"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZuxiDxmqvI_"
      },
      "source": [
        "# Loading Tokenizer, Config and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vPSV7fCEquZn"
      },
      "outputs": [],
      "source": [
        "def get_tokenier(special_tokens=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
        "\n",
        "    if special_tokens:\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "        print(len(tokenizer))\n",
        "        print(\"Special tokens added\")\n",
        "    return tokenizer\n",
        "\n",
        "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
        "\n",
        "    #GPT2LMHeadModel\n",
        "    if special_tokens:\n",
        "        config = AutoConfig.from_pretrained(MODEL, \n",
        "                          bos_token_id=tokenizer.bos_token_id,\n",
        "                          eos_token_id=tokenizer.eos_token_id,\n",
        "                          sep_token_id=tokenizer.sep_token_id,\n",
        "                          pad_token_id=tokenizer.pad_token_id,\n",
        "                          output_hidden_states=False)\n",
        "    else: \n",
        "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
        "                          pad_token_id=tokenizer.eos_token_id,\n",
        "                          output_hidden_states=False)    \n",
        "\n",
        "    #----------------------------------------------------------------#\n",
        "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
        "\n",
        "    if special_tokens:\n",
        "        #Special tokens added, model needs to be resized accordingly\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if load_model_path:\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "\n",
        "    model.cuda()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yKLw1xVq7qyQ"
      },
      "outputs": [],
      "source": [
        "def join_keywords(keywords, randomize=True):\n",
        "        N = len(keywords)\n",
        "\n",
        "        #random sampling and shuffle\n",
        "        if randomize: \n",
        "            M = random.choice(range(N+1))\n",
        "            keywords = keywords[:M]\n",
        "            random.shuffle(keywords)\n",
        "\n",
        "        return ','.join(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK8gR7cQMzR-"
      },
      "source": [
        "# Download Spear Email model - GPT2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bLNqOG_P7lN",
        "outputId": "e9968443-ec32-46ca-a384-5dd15e3c88d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=6b3ea9bf422a4f38d555a8f4b9e13d324377f2ea03c7a897d33fe617905ab088\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rd2e7hud/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "26pAeMDaMyme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483bc436-d440-43d3-d984-d67dc7334bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dPhbShbTwBB_3cdsV4zp0gLFD0mnREIO\n",
            "To: /content/pytorch_model.bin\n",
            "100% 1.44G/1.44G [00:45<00:00, 31.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1dPhbShbTwBB_3cdsV4zp0gLFD0mnREIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x37IhPq6KrBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4f183d714e884ef480af6825da0348b4",
            "0deaf3710b564b47b0b6a33dfb97510a",
            "4efc51e68718447e90cd5a7db0254c18",
            "17c5f4962b9641398787eebd1b4ba7cd",
            "963644203f26419e822cebb95731e3ec",
            "91bd804a6f5b42199fb9ee4d6e747ae6",
            "df141ba9ead3462a8b176f4b60ab24d7",
            "53ddd177428f45309b2938a0326f804c",
            "bbcf5b4c38764f369f38769d0bfbd2b0",
            "be022a96c12d4220b1c433e96400cc65",
            "e20b54c0cbbe40c5956bb068f274e349"
          ]
        },
        "outputId": "c31faaa3-2018-4146-cc57-e9730eb7d2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50262\n",
            "Special tokens added\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f183d714e884ef480af6825da0348b4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "\n",
        "model = get_model(tokenizer, \n",
        "          special_tokens=SPECIAL_TOKENS,\n",
        "          load_model_path='pytorch_model.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKjge7tP9y0G"
      },
      "source": [
        "Title => 文體起始句\n",
        "\n",
        "type => 類型\n",
        " * Fraud  : 詐騙/犯罪類型\n",
        " * Normal : 一般郵件\n",
        "\n",
        "category => 內文主題\n",
        " * 'CRIME' ,'ENTERTAINMENT', 'POLITICS', 'SPORTS', 'BUSINESS', 'TECH', 'EDUCATION',\n",
        " 'HEALTHY LIVING', 'MONEY', 'CULTURE & ARTS'\n",
        "\n",
        "formats => 文體撰寫手法 \n",
        " * News 新聞\n",
        " * Email 郵件\n",
        "\n",
        "keywords => 內文希望出現的關鍵字"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VervEmdaej7w"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NpbvBZ5ktQb"
      },
      "source": [
        "## Define Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_Symbol(s):\n",
        "    s = re.sub(r'[^\\w]','',s)\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "mfGene5-SdNM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pdTG2Q-nfaFi"
      },
      "outputs": [],
      "source": [
        "def generation_token(title,text,keywords,types,category,formats):    \n",
        "    \n",
        "    kw = join_keywords(keywords, randomize=False)\n",
        "    prompt = SPECIAL_TOKENS['bos_token'] + types + \\\n",
        "             SPECIAL_TOKENS['sep_token'] + category + \\\n",
        "             SPECIAL_TOKENS['sep_token'] + formats + \\\n",
        "             SPECIAL_TOKENS['sep_token'] + title + \\\n",
        "             SPECIAL_TOKENS['sep_token'] + kw + \\\n",
        "             SPECIAL_TOKENS['sep_token']\n",
        "    \n",
        "    prompt_len = len(types)+len(category)+len(formats)+ len(title) + len(','.join(keywords)) \n",
        "    \n",
        "    generated = tokenizer([prompt],max_length=MAXLEN, return_tensors='pt')['input_ids'].to(device)\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    sample_outputs = model.generate(\n",
        "        generated,\n",
        "        do_sample=False, \n",
        "        max_length=MAXLEN, \n",
        "        top_k=1,\n",
        "        top_p=0.75,\n",
        "        num_return_sequences=3\n",
        "    )\n",
        "    \n",
        "    return sample_outputs,len(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generation phishing emails for fraud"
      ],
      "metadata": {
        "id": "cShQMiYIU6dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic for COIVD19 of Fraud Emails"
      ],
      "metadata": {
        "id": "t8pxFR7jgxcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"please reply to me soon \"\n",
        "#title = \"please complete as soon as possible \"\n",
        "#title = 'I will get back to you in the am '\n",
        "#title = 'We are keenly interested in setting up a new hospital '\n",
        "#title = 'How about was that payment? reply to me soon '\n",
        "#title = \"I will get back to you in the am \"\n",
        "\n",
        "keywords = ['covid19', 'case', 'hospital','CDC']"
      ],
      "metadata": {
        "id": "ad2UAeeig5v1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = \"Fraud\"\n",
        "category = \"BUSINESS\"\n",
        "formats = \"Email\"\n",
        "kw = join_keywords(keywords, randomize=False)\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + types + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + category + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + formats + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + title + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "L5v9p6hugyKq"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                do_sample=True,   \n",
        "                min_length=20, \n",
        "                max_length=200,\n",
        "                top_k=10,                              \n",
        "                top_p=0.5,\n",
        "                repetition_penalty=2.0,\n",
        "                num_return_sequences=3\n",
        "                )\n",
        "\n",
        "predt_email = []\n",
        "prompt = len(types)+len(category)+len(formats)+ len(title) + len(kw) \n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  predit_text = tokenizer.decode(sample_output, skip_special_tokens=True)[prompt:]\n",
        "  index = predit_text.find('.')\n",
        "  result = \" \".join(colored(t,'white','on_red') if remove_Symbol(t) in keywords else remove_Symbol(t) for t in predit_text[index+1:].lower().split())\n",
        "  print('Phishing Email : {}'.format(i+1))\n",
        "  print(\"=\"*100)\n",
        "  for j,t in enumerate(result.split(' ')):\n",
        "    print(t+' ',end='')\n",
        "    if j%20==0 and j!=0:\n",
        "      print('\\n')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ep4VZ4ke6k",
        "outputId": "ea623ffd-a52a-4416-c305-ddbd840773e1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phishing Email : 1\n",
            "====================================================================================================\n",
            "i will be very glad if you can give your time and attention for a mutually beneficial business venturei am drdaniel \n",
            "\n",
            "j the director in charge of auditing  accounting section at cdcnyscavid19casedrauditorsdealingbankbankplease please reply to me soon \n",
            "\n",
            "Phishing Email : 2\n",
            "====================================================================================================\n",
            "i am drdavid covids a senior staff in the cdcs office of medical records and information technology mdr sectioni have urgent \n",
            "\n",
            "need for an immediate foreign partner who can assist us with this transaction as we are still recovering from our \n",
            "\n",
            "recent hospitalization due on \u001b[41m\u001b[37mcovid-19.\u001b[0m my name is david cevides mdthe chief executive officerof dbe international consortium ltdcavid20 please reply \n",
            "\n",
            "urgently to me soon \n",
            "\n",
            "Phishing Email : 3\n",
            "====================================================================================================\n",
            "joseph_mccoyyahoocomi am contacting you on a business transaction that will be of great interest and reward for bothof us i have \n",
            "\n",
            "been diagnosed with \u001b[41m\u001b[37mcovid19\u001b[0m a rare genetic condition which requires immediate medical attention and hospitalizationcavidsdocumentedbankdrjosephdear please reply urgently my name \n",
            "\n",
            "is dr joseph mccorry the director in charge heading departmentdepartment of health  social services in this department we discovered \n",
            "\n",
            "an abandoned sum 20000 usd297million dollars us dollars deposited into our bank by one mrdavid sosnitsky who died after being \n",
            "\n",
            "involvedin car accident alongwith his passengers during their brief stay at home before he was recovered from death in january \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic for Finace of Fraud Emails"
      ],
      "metadata": {
        "id": "tovBxbqegxv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'How was that payment? reply to me soon '\n",
        "#title = 'I will get back to you in the am '\n",
        "#title = 'please, reply to me soon '\n",
        "keywords = ['bank','account','number','reply','credit']"
      ],
      "metadata": {
        "id": "nUJ0PnTagpF6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = \"Fraud\" \n",
        "category = \"BUSINESS\"\n",
        "formats = \"Email\"\n",
        "kw = join_keywords(keywords, randomize=False)\n",
        "\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + types + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + category + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + formats + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + title + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "ir3_sUNfUx6S"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                do_sample=True,   \n",
        "                min_length=20, \n",
        "                max_length=200,\n",
        "                top_k=10,                              \n",
        "                top_p=0.5,\n",
        "                repetition_penalty=2.0,\n",
        "                num_return_sequences=3\n",
        "                )\n",
        "\n",
        "predt_email = []\n",
        "prompt = len(types)+len(category)+len(formats)+ len(title) + len(kw) \n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  predit_text = tokenizer.decode(sample_output, skip_special_tokens=True)[prompt:]\n",
        "  index = predit_text.find('.')\n",
        "  result = \" \".join(colored(t,'white','on_red') if remove_Symbol(t) in keywords else remove_Symbol(t) for t in predit_text[index+1:].lower().split())\n",
        "  print('Phishing Email : {}'.format(i+1))\n",
        "  print(\"=\"*100)\n",
        "  for j,t in enumerate(result.split(' ')):\n",
        "    print(t+' ',end='')\n",
        "    if j%20==0 and j!=0:\n",
        "      print('\\n')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1j2_5FBZ5z9",
        "outputId": "e80d0878-0e66-4e01-d938-43d529ddf895"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phishing Email : 1\n",
            "====================================================================================================\n",
            "i am contacting you because i want your help and assistance in a matter of timemy name is john doe and \n",
            "\n",
            "i work with the international operation departmentof one foreign firm in nigeriabankaccountsjohnafricahow was that payment made for \u001b[41m\u001b[37mcredit\u001b[0m reasons or \n",
            "\n",
            "on behalf thereof \u001b[41m\u001b[37mreply\u001b[0m my email belowhttpmail2yahoocommail_promo1html dear friendplease permit this brief introduction before we proceedbankaddressbanka0why did he pay his \n",
            "\n",
            "bill through an \u001b[41m\u001b[37maccount\u001b[0m opened by him without prior consent from us please be patient as it takes sometimefor our \n",
            "\n",
            "\u001b[41m\u001b[37mbank\u001b[0m director the manager who will have all necessary information regardingthis transaction sothatyou can make sure everything goes wellwith \n",
            "\n",
            "Phishing Email : 2\n",
            "====================================================================================================\n",
            "i am contacting you because of a financial emergency and the need for immediate assistance from my \u001b[41m\u001b[37mbank\u001b[0m in regardsof this \n",
            "\n",
            "accountwhich is presently pending or any other fundsthat may beinvolving itplease do not hesitate at usingthis medium asap to reach \n",
            "\n",
            "outfor help andor investment which i believe willbe very beneficial if we can jointlyestablish aninvestment venture togetheri have decidednot remailing \n",
            "\n",
            "through email due security reasons but by going straight viafaxemailbankaccountsauvergnebankafricahow did your \u001b[41m\u001b[37mcredit\u001b[0m rating fall below 3 stars on one \n",
            "\n",
            "particular branch herein africa \u001b[41m\u001b[37mreply\u001b[0m back immediately witha private telephone \u001b[41m\u001b[37mnumber\u001b[0m soas never beforethis information should remain confidentialand unconfidential unless \n",
            "\n",
            "\n",
            "\n",
            "Phishing Email : 3\n",
            "====================================================================================================\n",
            "i am the \u001b[41m\u001b[37mcredit\u001b[0m officer of a \u001b[41m\u001b[37mbank\u001b[0m here in londoni have an urgent and very confidential business proposition for youon \n",
            "\n",
            "june 6 2000 my late father deposited with us 25000 000 pounds into our \u001b[41m\u001b[37maccount\u001b[0m \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generation phishing emails for BEC"
      ],
      "metadata": {
        "id": "VonB6CikVz1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic for COIVD19 of BEC Emails"
      ],
      "metadata": {
        "id": "FjZxIM7hmGYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"please reply to me soon \"\n",
        "#title = \"please complete as soon as possible \"\n",
        "#title = 'I will get back to you in the am '\n",
        "#title = 'We are keenly interested in setting up a new hospital '\n",
        "#title = 'How about was that payment? reply to me soon '\n",
        "#title = \"I will get back to you in the am \"\n",
        "types = \"Normal\" #犯罪的文章，包含詐騙郵件詞表\n",
        "#types = \"Normal\" \n",
        "category = \"BUSINESS\" #商務文章，包含正常郵件詞表 \n",
        "formats = \"Email\" #Email文體\n",
        "\n",
        "#情境1 面向職員秘書助理\n",
        "#keywords = ['read', 'bank', 'account', 'reply']\n",
        "#keywords = ['officer', 'attached', 'account','bank','credit']\n",
        "#keywords = ['officer', 'attached', 'amount','bank','credit']\n",
        "#keywords = ['bank','account','reply','number','credit']\n",
        "#情境2 面向醫師高階管職\n",
        "keywords = ['covid19', 'case', 'hospital','CDC']\n",
        "kw = join_keywords(keywords, randomize=False)\n",
        "\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + types + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + category + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + formats + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + title + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "cCeDCORWV41r"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                do_sample=True,   \n",
        "                min_length=20, \n",
        "                max_length=200,\n",
        "                top_k=10,                              \n",
        "                top_p=0.5,\n",
        "                repetition_penalty=2.0,\n",
        "                num_return_sequences=3\n",
        "                )\n",
        "\n",
        "predt_email = []\n",
        "prompt = len(types)+len(category)+len(formats)+ len(title) + len(kw) \n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  predit_text = tokenizer.decode(sample_output, skip_special_tokens=True)[prompt:]\n",
        "  index = predit_text.find('.')\n",
        "  result = \" \".join(colored(t,'white','on_red') if remove_Symbol(t) in keywords else remove_Symbol(t) for t in predit_text[index+1:].lower().split())\n",
        "  print('Phishing Email : {}'.format(i+1))\n",
        "  print(\"=\"*100)\n",
        "  for j,t in enumerate(result.split(' ')):\n",
        "    print(t+' ',end='')\n",
        "    if j%20==0 and j!=0:\n",
        "      print('\\n')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "4jVqoO3eV3hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b57c1f-a728-43ba-924a-94f3df18a9e9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phishing Email : 1\n",
            "====================================================================================================\n",
            "i will be in dc until monday at which time you can send the letter and well discuss how best proceed \n",
            "\n",
            "with a formalletterconfirmation that is bothlegal routine for us as wellasfor your colleagues on capitol hill \n",
            "\n",
            "Phishing Email : 2\n",
            "====================================================================================================\n",
            "im in the \u001b[41m\u001b[37mhospital\u001b[0m recovering from a stroke and am on ventilators for at least three days the doctors are treating \n",
            "\n",
            "it as an acute \u001b[41m\u001b[37mcase\u001b[0m of pneumonia so theyre trying everything possible to keep him alive \n",
            "\n",
            "Phishing Email : 3\n",
            "====================================================================================================\n",
            "4please respondtome now sooner i will have a new draft of the letter that you can send in my direction asap \n",
            "\n",
            "and then we could discuss it further with him at his \u001b[41m\u001b[37mhospital\u001b[0m room on thursday morning afternoon before he goes \n",
            "\n",
            "home for lunchtime or dinner time so i dont wantyou wasting your energy trying this again if there is something \n",
            "\n",
            "else goingon here and hope everyone getswell after all \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic for finance of BEC Emails"
      ],
      "metadata": {
        "id": "fmv8mg_bmEzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'How was that payment? reply to me soon '\n",
        "#title = 'I will get back to you in the am '\n",
        "#title = 'please, reply to me soon '\n",
        "keywords = ['bank','account','number','reply','credit']\n",
        "#keywords = ['bank','account','reply','number','credit']"
      ],
      "metadata": {
        "id": "Fsm_qqacmEcI"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = \"Normal\" #犯罪的文章，包含詐騙郵件詞表\n",
        "#types = \"Normal\" \n",
        "category = \"BUSINESS\" #商務文章，包含正常郵件詞表 \n",
        "formats = \"Email\" #Email文體\n",
        "\n",
        "kw = join_keywords(keywords, randomize=False)\n",
        "\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + types + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + category + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + formats + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + title + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw\n",
        "print(prompt)\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "print(generated)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "VV2CrBRukQAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a8f3aa-cc7f-47cc-983b-b860577e7315"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|BOS|>Normal<|SEP|>BUSINESS<|SEP|>Email<|SEP|>How was that payment? reply to me soon <|SEP|>bank,account,number,reply,credit\n",
            "tensor([[50257, 26447, 50261, 45346, 44180, 50261, 15333, 50261,  2437,   373,\n",
            "           326,  6074,    30, 10971,   284,   502,  2582,   220, 50261, 17796,\n",
            "            11, 23317,    11, 17618,    11, 47768,    11, 43082]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from termcolor import colored\n",
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                do_sample=True,   \n",
        "                min_length=20, \n",
        "                max_length=200,\n",
        "                top_k=10,                              \n",
        "                top_p=0.5,\n",
        "                repetition_penalty=2.0,\n",
        "                num_return_sequences=3\n",
        "                )\n",
        "\n",
        "predt_email = []\n",
        "prompt = len(types)+len(category)+len(formats)+ len(title) + len(kw) \n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  predit_text = tokenizer.decode(sample_output, skip_special_tokens=True)[prompt:]\n",
        "  index = predit_text.find('.')\n",
        "  result = \" \".join(colored(t,'white','on_red') if remove_Symbol(t) in keywords else remove_Symbol(t) for t in predit_text[index+1:].lower().split())\n",
        "  print('Phishing Email : {}'.format(i+1))\n",
        "  print(\"=\"*100)\n",
        "  for j,t in enumerate(result.split(' ')):\n",
        "    print(t+' ',end='')\n",
        "    if j%20==0 and j!=0:\n",
        "      print('\\n')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdhaT4qyeyqC",
        "outputId": "d684f4f0-3bb4-4506-96ce-fdc705ceea15"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phishing Email : 1\n",
            "====================================================================================================\n",
            "and my answer is simple its because we are dealing with an \u001b[41m\u001b[37maccount\u001b[0m \u001b[41m\u001b[37mnumber\u001b[0m of 20 million dollars which has no \n",
            "\n",
            "interest rate on its face \n",
            "\n",
            "Phishing Email : 2\n",
            "====================================================================================================\n",
            "\u001b[41m\u001b[37mreply\u001b[0m and we can discuss how bestto pay this bill or if you want us go after another lenderbusiness which is \n",
            "\n",
            "more likely to be of interest than your current financial situation \n",
            "\n",
            "Phishing Email : 3\n",
            "====================================================================================================\n",
            "i have not yet heard from them but if there is any way forward we will be very gratefulif one can \n",
            "\n",
            "come along who hasthe capacityinterests necessary at our disposal to assist us \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GPT2_Generation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f183d714e884ef480af6825da0348b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0deaf3710b564b47b0b6a33dfb97510a",
              "IPY_MODEL_4efc51e68718447e90cd5a7db0254c18",
              "IPY_MODEL_17c5f4962b9641398787eebd1b4ba7cd"
            ],
            "layout": "IPY_MODEL_963644203f26419e822cebb95731e3ec"
          }
        },
        "0deaf3710b564b47b0b6a33dfb97510a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91bd804a6f5b42199fb9ee4d6e747ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_df141ba9ead3462a8b176f4b60ab24d7",
            "value": "Downloading: 100%"
          }
        },
        "4efc51e68718447e90cd5a7db0254c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53ddd177428f45309b2938a0326f804c",
            "max": 1520013706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbcf5b4c38764f369f38769d0bfbd2b0",
            "value": 1520013706
          }
        },
        "17c5f4962b9641398787eebd1b4ba7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be022a96c12d4220b1c433e96400cc65",
            "placeholder": "​",
            "style": "IPY_MODEL_e20b54c0cbbe40c5956bb068f274e349",
            "value": " 1.52G/1.52G [01:21&lt;00:00, 23.8MB/s]"
          }
        },
        "963644203f26419e822cebb95731e3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91bd804a6f5b42199fb9ee4d6e747ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df141ba9ead3462a8b176f4b60ab24d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53ddd177428f45309b2938a0326f804c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcf5b4c38764f369f38769d0bfbd2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be022a96c12d4220b1c433e96400cc65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e20b54c0cbbe40c5956bb068f274e349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}